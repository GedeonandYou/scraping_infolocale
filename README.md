# Infolocale Scraper

Syst√®me complet de scraping et d'API REST pour collecter et exposer les √©v√©nements du site [infolocale.fr](https://www.infolocale.fr).

## üìã Table des mati√®res

- [Caract√©ristiques](#caract√©ristiques)
- [Architecture](#architecture)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [API REST](#api-rest)
- [Tests](#tests)
- [Documentation](#documentation)

## ‚ú® Caract√©ristiques

### MVP (Minimum Viable Product)
- ‚úÖ Scraping automatis√© des √©v√©nements depuis Infolocale.fr
- ‚úÖ Pagination automatique
- ‚úÖ Stockage PostgreSQL avec d√©duplication (champ `uid`)
- ‚úÖ G√©ocodage via Google Places API (latitude/longitude)
- ‚úÖ Logging complet des op√©rations
- ‚úÖ Export CSV et JSON
- ‚úÖ API REST FastAPI avec documentation Swagger
- ‚úÖ Docker Compose (PostgreSQL + Adminer + API)

### Fonctionnalit√©s avanc√©es
- Rate limiting respectueux
- Retry automatique avec backoff exponentiel
- Cache des r√©sultats de g√©ocodage
- Interface CLI interactive (Typer + Rich)
- Statistiques et m√©triques

## üèó Architecture

```
src/
‚îú‚îÄ‚îÄ api/              # Endpoints FastAPI
‚îú‚îÄ‚îÄ models/           # Mod√®les SQLModel (ORM)
‚îú‚îÄ‚îÄ schemas/          # Sch√©mas Pydantic (validation)
‚îú‚îÄ‚îÄ services/         # Logique m√©tier
‚îÇ   ‚îú‚îÄ‚îÄ scraper_service.py
‚îÇ   ‚îú‚îÄ‚îÄ geocoding_service.py
‚îÇ   ‚îî‚îÄ‚îÄ storage_service.py
‚îú‚îÄ‚îÄ exporters/        # Export CSV/JSON
‚îú‚îÄ‚îÄ utils/            # Utilitaires (logging)
‚îî‚îÄ‚îÄ config/           # Configuration

docker/               # Fichiers Docker
tests/                # Tests unitaires et d'int√©gration
data/exports/         # Donn√©es export√©es
logs/                 # Logs applicatifs
```

## üì¶ Pr√©requis

- Python 3.10+
- Docker & Docker Compose
- Cl√© API Google Places (pour le g√©ocodage)

## üöÄ Installation

### 1. Cloner le projet

```bash
git clone <repo_url>
cd scraping_infolocale
```

### 2. Configuration de l'environnement

```bash
cp .env.example .env
```

√âditer le fichier `.env` :
```env
# Database
POSTGRES_USER=infolocale_user
POSTGRES_PASSWORD=votre_mot_de_passe_securise
POSTGRES_DB=infolocale_db

# Google Places API
GOOGLE_PLACES_API_KEY=votre_cle_api_google

# Scraping
SCRAPING_DELAY=2
SCRAPING_USER_AGENT=InfoLocaleScraper/1.0 (Educational Project; contact@example.com)
```

### 3. D√©marrage avec Docker Compose

```bash
docker-compose up -d
```

Services disponibles :
- **PostgreSQL** : `localhost:5432`
- **Adminer** (UI DB) : http://localhost:8080
- **API FastAPI** : http://localhost:8000
- **Documentation Swagger** : http://localhost:8000/docs

### 4. Installation locale (alternative)

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

pip install -r requirements.txt
```

## ‚öôÔ∏è Configuration

### Variables d'environnement

Voir [.env.example](.env.example) pour la liste compl√®te des variables configurables.

### Google Places API

1. Cr√©er un projet sur [Google Cloud Console](https://console.cloud.google.com/)
2. Activer l'API "Places API"
3. Cr√©er une cl√© API
4. Ajouter la cl√© dans `.env` : `GOOGLE_PLACES_API_KEY=...`

## üíª Utilisation

### Interface CLI

```bash
# Initialiser la base de donn√©es
python main.py init-db

# Lancer le scraping
python main.py scrape --max-pages 10 --geocode

# Exporter les donn√©es
python main.py export --format json
python main.py export --format csv
python main.py export --format all --limit 100

# Afficher les statistiques
python main.py stats

# Lancer l'API
python main.py serve --host 0.0.0.0 --port 8000
```

### API REST

```bash
# D√©marrer l'API
uvicorn src.main:app --reload

# Ou via le CLI
python main.py serve
```

Acc√©der √† la documentation interactive :
- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

## üîå API REST

### Endpoints principaux

#### √âv√©nements

```bash
# Liste des √©v√©nements (avec pagination et filtres)
GET /api/v1/events?page=1&page_size=20&city=Paris&category=Concert

# D√©tail d'un √©v√©nement
GET /api/v1/events/{event_id}

# Cr√©er un √©v√©nement
POST /api/v1/events

# Mettre √† jour un √©v√©nement
PATCH /api/v1/events/{event_id}

# Supprimer un √©v√©nement
DELETE /api/v1/events/{event_id}
```

#### M√©tadonn√©es

```bash
# Liste des cat√©gories
GET /api/v1/categories

# Liste des villes
GET /api/v1/cities?state=Bretagne

# Statistiques
GET /api/v1/stats
```

### Exemples de requ√™tes

```bash
# R√©cup√©rer les √©v√©nements √† Paris
curl "http://localhost:8000/api/v1/events?city=Paris&page=1&page_size=10"

# Statistiques
curl "http://localhost:8000/api/v1/stats"
```

## üß™ Tests

```bash
# Installer les d√©pendances de test
pip install -r requirements.txt

# Lancer les tests
pytest

# Avec couverture
pytest --cov=src --cov-report=html

# Tests sp√©cifiques
pytest tests/unit/
pytest tests/integration/
```

## üìä Mod√®le de donn√©es

### Table `scanned_events`

Conforme au cahier des charges (section 5) :

| Champ | Type | Description |
|-------|------|-------------|
| `id` | SERIAL | Cl√© primaire |
| `user_id` | INTEGER | FK vers `users(id)` |
| `uid` | VARCHAR(100) | Identifiant unique (d√©duplication) |
| `title` | VARCHAR(500) | Titre de l'√©v√©nement |
| `category` | VARCHAR(255) | Cat√©gorie |
| `begin_date` | DATE | Date de d√©but |
| `description` | TEXT | Description |
| `city` | VARCHAR(200) | Ville |
| `latitude` | DOUBLE PRECISION | Latitude GPS |
| `longitude` | DOUBLE PRECISION | Longitude GPS |
| `place_id` | VARCHAR(255) | Google Place ID |
| ... | ... | (voir [docker/init.sql](docker/init.sql)) |

### Index optimis√©s

- `idx_scanned_events_user` : sur `user_id`
- `idx_scanned_events_private` : sur `is_private`
- `idx_scanned_events_coords` : sur `(latitude, longitude)`
- `idx_scanned_events_uid` : sur `uid` (d√©duplication)
- `idx_scanned_events_city` : sur `city`

## üìö Documentation

- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** : Sch√©ma d'architecture d√©taill√©
- **[DATABASE.md](docs/DATABASE.md)** : Sch√©ma de la base de donn√©es
- **[LEGAL.md](docs/LEGAL.md)** : Aspects l√©gaux et √©thiques du scraping
- **[API.md](docs/API.md)** : Documentation compl√®te de l'API

## üõ° Aspects l√©gaux

Ce projet respecte :
- ‚úÖ `robots.txt` du site cible
- ‚úÖ Rate limiting (d√©lai de 2s entre requ√™tes)
- ‚úÖ User-Agent identifiable
- ‚úÖ Pas de collecte de donn√©es personnelles
- ‚úÖ Respect du RGPD

**Note** : Infolocale propose des donn√©es ouvertes sur [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/donnees-evenementielles-infolocale/) (licence ODbL).

## ü§ù Contribution

Ce projet est r√©alis√© dans un cadre p√©dagogique. Pour toute question ou suggestion, ouvrir une issue.

## üìÑ Licence

Projet √©ducatif - Tous droits r√©serv√©s.

## üë®‚Äçüíª Auteur

Projet Scraping Infolocale - Janvier 2026

---

## üöÄ Quick Start

```bash
# 1. Configuration
cp .env.example .env
# √âditer .env avec vos credentials

# 2. D√©marrer les services Docker
docker compose up -d

# 3. Initialiser la base de donn√©es
python main.py init-db

# 4. Importer les donn√©es (CSV d'exemple)
python main.py import-opendata --csv-path data/example_events.csv

# 5. Voir les stats
python main.py stats

# 6. Lancer l'API
python main.py serve

# 7. Acc√©der √† la documentation
# http://localhost:8000/docs
```

**‚ö†Ô∏è Note** : Les donn√©es Open Data d'Infolocale ne sont plus accessibles via l'ancienne API.
Consultez [DATASOURCES.md](DATASOURCES.md) pour les alternatives.

## üìû Support

- Documentation : http://localhost:8000/docs
- Adminer (DB UI) : http://localhost:8080
- Logs : `logs/scraper.log`
